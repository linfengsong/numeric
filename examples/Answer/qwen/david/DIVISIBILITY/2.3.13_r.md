<think>
Okay, so I need to prove two things about integers a, b, and c. Let me start with part (a). The problem says that there exist integers x and y such that c = ax + by if and only if gcd(a, b) divides c. Hmm, okay. So I need to show that two things are equivalent: the existence of such x and y, and the divisibility condition.

First, maybe I should recall what the greatest common divisor (gcd) of two numbers means. The gcd of a and b is the largest integer that divides both a and b. Also, I remember something called Bézout's identity, which states that for any integers a and b, there exist integers x and y such that ax + by = gcd(a, b). So that seems related. But here, the problem is about c being a linear combination of a and b, not necessarily the gcd. 

So, the "if and only if" statement is saying that c can be written as a linear combination of a and b exactly when gcd(a, b) divides c. So, maybe I need to prove both directions: if gcd(a, b) divides c, then there are x and y such that c = ax + by, and conversely, if such x and y exist, then gcd(a, b) divides c.

Let me start with the first direction: if gcd(a, b) divides c, then there exist integers x and y such that c = ax + by. Well, since gcd(a, b) divides both a and b, then any linear combination of a and b will be a multiple of gcd(a, b). So if c is a multiple of gcd(a, b), then c is in the set of all such linear combinations. But how do I actually construct x and y?

Alternatively, maybe I can use the fact that if d = gcd(a, b), then we can write a = d*a', b = d*b', where a' and b' are coprime. Then, if c is a multiple of d, say c = d*c', then we can write c = d*c' = d*(a'*x + b'*y) = a*(x) + b*(y) where x and y are such that a'x + b'y = c'. Since a' and b' are coprime, by Bézout's identity, there exist integers x and y such that a'x + b'y = 1. So multiplying both sides by c', we get a'*(c')x + b'*(c')y = c', hence d*(a'*(c')x + b'*(c')y) = d*c' = c. Therefore, c = a*(c'x) + b*(c'y). Therefore, such x and y exist. So that seems to work. But maybe I need to formalize this more carefully.

Alternatively, maybe I can think about the ideal generated by a and b in the integers. The set of all linear combinations ax + by is an ideal, and in the integers, the only ideals are generated by the gcd. So, the ideal generated by a and b is equal to the ideal generated by gcd(a, b). Therefore, any element in that ideal must be a multiple of gcd(a, b). Therefore, c is in the ideal if and only if gcd(a, b) divides c. Therefore, c can be written as ax + by iff gcd(a, b) divides c. That's a more abstract approach, but maybe that's acceptable?

But maybe I need to do it more concretely. Let me try again.

First, suppose that gcd(a, b) divides c. Let d = gcd(a, b). Then, since d divides a and d divides b, then d divides any linear combination of a and b. Therefore, if c is a multiple of d, then c is a linear combination of a and b. But how do I show that there exist specific x and y?

Alternatively, since d divides c, we can write c = d*k for some integer k. Then, since d divides a and d divides b, we can write a = d*a', b = d*b', where a' and b' are integers. Then, c = d*k = d*a'*x + d*b'*y => k = a'*x + b'*y. Since a' and b' are coprime, by Bézout's identity, there exist integers x and y such that a'*x + b'*y = 1. Therefore, multiplying both sides by k, we get a'*kx + b'*ky = k. Therefore, substituting back, c = d*(a'*kx + b'*ky) = a*(kx) + b*(ky). Therefore, x = kx and y = ky. Therefore, such integers exist. Therefore, the first direction is proven.

Now, the converse: if there exist integers x and y such that c = ax + by, then d = gcd(a, b) divides c. Well, since d divides a and d divides b, then d divides any linear combination of a and b. Therefore, d divides ax + by = c. Therefore, d divides c. Therefore, that's the converse.

Therefore, both directions are proven, so the equivalence holds. Therefore, part (a) is done.

Now, moving on to part (b): If there exist integers x and y for which ax + by = gcd(a, b), then gcd(x, y) = 1.

So, given that ax + by = gcd(a, b), we need to show that gcd(x, y) = 1. Let me think. Let me denote d = gcd(a, b). Then, the equation is ax + by = d. Let me denote g = gcd(x, y). Then, we need to show that g = 1.

Suppose that g > 1. Then, g divides x and g divides y. Therefore, g divides ax + by. But ax + by = d, so g divides d. But d is the gcd of a and b, so d is the greatest common divisor. Therefore, any common divisor of x and y must divide d. However, since d is the gcd of a and b, then if g divides d, then g divides a and g divides b. But since ax + by = d, and g divides ax + by, which is d, then g divides d. However, if g divides both x and y, then g divides ax + by = d. But how does that lead to a contradiction?

Wait, but maybe we can think in terms of the gcd of x and y. Let me try to suppose that g = gcd(x, y) > 1. Then, there exists some prime p that divides both x and y. Therefore, p divides ax + by. But ax + by = d, so p divides d. Therefore, p divides d. But d is the gcd of a and b, so p divides a and p divides b. But since p divides x and y, then p divides ax + by = d. However, if p divides a and p divides x, then p divides ax. Similarly, p divides by. Therefore, p divides ax + by = d. But since p divides d, which is the gcd of a and b, that is okay. But how does this lead to a contradiction?

Wait, maybe I need to think about the fact that if p divides both x and y, then p divides d. But since d is the gcd of a and b, then p divides a and p divides b. However, if p divides x and y, then since ax + by = d, then p divides d. But if p divides d, then p divides a and p divides b. However, if p divides x and y, then perhaps we can find a contradiction?

Wait, maybe not. Let me think again. Let me suppose that there exists a prime p > 1 that divides both x and y. Then, since p divides x and y, then p divides ax + by = d. Therefore, p divides d. Therefore, p divides gcd(a, b). Therefore, p divides a and p divides b. However, since p divides x, then p divides ax. Similarly, p divides by. Therefore, p divides ax + by = d. But since p divides d, that's okay. But how does this lead to a contradiction? Maybe there is a way to show that such p cannot exist?

Alternatively, maybe we can use the fact that if ax + by = d, then gcd(x, y) divides d. Wait, but since d is the gcd of a and b, and if we have that gcd(x, y) divides d, but we need to show that gcd(x, y) is 1. So maybe if we suppose that gcd(x, y) = g > 1, then g divides d. But since d is the gcd of a and b, then g divides a and g divides b? Wait, no. Wait, if g divides x and y, then g divides ax + by = d, so g divides d. But that doesn't necessarily mean that g divides a or b. Wait, but if g divides d, which is the gcd of a and b, then g divides a and g divides b. Wait, no. Wait, if d is the gcd of a and b, then any divisor of d divides both a and b. Therefore, if g divides d, then g divides a and g divides b. Therefore, if g divides x and y, then g divides d, so g divides a and g divides b. Therefore, g divides a and g divides b, and g divides x and y. But then, since g divides a and g divides x, then g divides ax. Similarly, g divides by. Therefore, g divides ax + by = d. Which is consistent. But how does that help?

Wait, but if g divides both a and b, and also divides x and y, then maybe there's a contradiction? Wait, maybe not. For example, take a = 2, b = 4, so d = 2. Then, ax + by = 2. Let me find x and y. For example, x = 1, y = 0: 2*1 + 4*0 = 2. Then, gcd(x, y) = gcd(1, 0) = 1. Another example: x = 0, y = 1/2? Wait, but x and y have to be integers. So maybe another example. Let me take a = 2, b = 4. Then, ax + by = 2. Let me solve 2x + 4y = 2. Divide both sides by 2: x + 2y = 1. Solutions are x = 1 - 2y. So for y = 0, x = 1. For y = 1, x = -1. So gcd(1, 0) = 1, gcd(-1, 1) = 1. So in this case, gcd(x, y) is 1. So maybe in this case, even though d = 2, and g divides d, but in this case, g is 1. So maybe there is no contradiction here. So perhaps my previous reasoning is missing something.

Alternatively, let me think about the equation ax + by = d. Suppose that g = gcd(x, y). Then, since g divides x and y, then g divides ax + by = d. Therefore, g divides d. But since d is the gcd of a and b, then g divides d. But also, since g divides x and y, then perhaps we can find a relation between g and the original numbers.

Alternatively, maybe consider that if g divides x and y, then we can write x = g*x', y = g*y'. Then, substituting into the equation ax + by = d, we get a*g*x' + b*g*y' = d => g*(a x' + b y') = d. Therefore, g divides d. But since d is the gcd of a and b, then d = gcd(a, b). Therefore, g divides d. However, since d is the gcd, then d is the greatest common divisor. Therefore, if g divides d, then g divides a and g divides b. Therefore, we have that g divides a and g divides b, and also g divides x and y. But then, since g divides a and g divides b, and g divides x and y, then maybe we can find a contradiction?

Wait, but if we have that g divides x and y, and g divides a and b, then perhaps we can find a smaller common divisor? Wait, but how?

Alternatively, maybe we can think of the equation ax + by = d. Suppose that g = gcd(x, y) > 1. Then, since g divides x and y, then g divides d. Therefore, let me write d = g * k. Then, the equation becomes ax + by = g * k. But since x = g * x', y = g * y', then substituting, we get a * g x' + b * g y' = g * k => g(a x' + b y') = g * k => a x' + b y' = k. Therefore, k = a x' + b y'. But k = d / g. Therefore, since d = gcd(a, b), then k = d / g. But since g divides d, then k is an integer.

But now, note that gcd(a, b) = d. Therefore, the equation a x' + b y' = k. But since d divides a and d divides b, then d divides k. Therefore, k = d / g must be divisible by d? Wait, no. Wait, k is d / g, and since d divides a and b, then k = d / g is an integer. But how does that relate?

Wait, but if we have a x' + b y' = k, and since d divides a and b, then d divides k. Therefore, d divides k. But k = d / g, so d divides d / g. Therefore, d divides d / g implies that g divides 1. Because if d divides d/g, then d/g must be a multiple of d, which would mean that g divides 1. Therefore, g divides 1, so g = 1. Therefore, this shows that g must be 1. Therefore, gcd(x, y) = 1. Therefore, that's the proof.

So, let me recap that. Suppose that g = gcd(x, y) > 1. Then, since g divides x and y, then g divides ax + by = d. Therefore, g divides d. Let d = g * k. Then, substituting x = g x', y = g y', we get a g x' + b g y' = g k => a x' + b y' = k. But since d = gcd(a, b), then d divides a and d divides b, so d divides k. Therefore, d divides k, but k = d / g. Therefore, d divides d / g => g divides 1. Therefore, g = 1. Therefore, contradiction. Therefore, our assumption that g > 1 is false, so g = 1. Therefore, gcd(x, y) = 1.

Therefore, part (b) is proven.

Wait, let me check with an example. Suppose a = 2, b = 4, so d = 2. Then, ax + by = 2. Let me take x = 1, y = 0. Then, gcd(1, 0) = 1. Another example, if a = 3, b = 6, then d = 3. Then, ax + by = 3. Let me take x = 1, y = 0. gcd(1, 0) = 1. If I take x = 0, y = 1/2, but that's not integer. Another solution is x = 1, y = 0. Another solution is x = -1, y = 1: 3*(-1) + 6*1 = -3 + 6 = 3. Then, gcd(-1, 1) = 1. So that works. Another example: a = 4, b = 6, d = 2. Then, ax + by = 2. Let me find x and y. For example, 4x + 6y = 2. Divide by 2: 2x + 3y = 1. Solutions: x = 2, y = -1. Then, gcd(2, -1) = 1. Another solution: x = -1, y = 1. gcd(-1, 1) = 1. So that works.

Another example where maybe x and y have a common divisor? Let me think. Suppose a = 2, b = 2. Then, gcd(a, b) = 2. Then, ax + by = 2. Let me take x = 1, y = 0. gcd(1, 0) = 1. If I take x = 0, y = 1. gcd(0, 1) = 1. If I take x = 2, y = 0: gcd(2, 0) = 2. Wait, but in this case, ax + by = 2*2 + 2*0 = 4, which is not equal to 2. Wait, so that's not a solution. Wait, let me find a solution. 2x + 2y = 2. Divide by 2: x + y = 1. So solutions are x = 1 - y. For example, x = 1, y = 0. Then, gcd(1, 0) = 1. If x = 0, y = 1, gcd(0, 1) = 1. If x = 2, y = -1, then gcd(2, -1) = 1. So even though a and b are both 2, the solutions x and y have gcd 1. Therefore, even if a and b have a common divisor, the x and y in the Bézout identity have gcd 1. Therefore, the example seems to hold.

Wait, but what if a and b are coprime? Let me take a = 3, b = 5, so d = 1. Then, ax + by = 1. For example, x = 2, y = -1: 3*2 + 5*(-1) = 6 -5 = 1. Then, gcd(2, -1) = 1. Another solution: x = -2, y = 1: gcd(-2, 1) = 1. So that works. So in all these examples, the gcd(x, y) is 1. Therefore, the proof seems to hold.

Therefore, part (b) is proven.

Therefore, both parts (a) and (b) are proven. Let me just check if there's any missing steps or possible errors in the reasoning.

For part (a), the key idea is that the set of all linear combinations of a and b is the ideal generated by gcd(a, b). Therefore, c is in that ideal if and only if gcd(a, b) divides c. The proof via Bézout's identity also works, as shown.

For part (b), the key idea is that if there is a common divisor of x and y, then it must divide d = gcd(a, b), but then substituting into the equation leads to a contradiction unless that divisor is 1.

Therefore, I think the proofs are correct.

**Final Answer**

(a) \boxed{\text{The statement is proven.}}  
(b) \boxed{1}
</think>

To address the problem, we consider two key number-theoretic facts: **Bézout's Identity** and the **properties of the greatest common divisor (gcd)**.

---

### **Part (a):**

**Statement:**  
There exist integers $ x $ and $ y $ such that $ c = ax + by $ **if and only if** $ \gcd(a, b) \mid c $.

---

#### **Proof:**

Let $ d = \gcd(a, b) $. By **Bézout’s Identity**, there exist integers $ x_0, y_0 $ such that:

$$
ax_0 + by_0 = d
$$

This implies that the set of all linear combinations $ ax + by $ is the set of all integer multiples of $ d $, i.e., the ideal generated by $ a $ and $ b $ in the ring of integers is equal to the ideal generated by $ d $.

Hence, an integer $ c $ can be expressed as $ ax + by $ **if and only if** $ d \mid c $.

---

**Conversely**, if $ d \mid c $, then $ c = dk $ for some integer $ k $. Since $ a = d \cdot a' $, $ b = d \cdot b' $, and $ \gcd(a', b') = 1 $, we can write:

$$
c = d \cdot k = d(a'x + b'y)
$$

So, $ c = ax + by $ with $ x = k a' $ and $ y = k b' $, which are integers. Therefore, the equivalence is established.

---

### **Part (b):**

**Statement:**  
If there exist integers $ x $ and $ y $ such that $ ax + by = \gcd(a, b) $, then $ \gcd(x, y) = 1 $.

---

#### **Proof:**

Let $ d = \gcd(a, b) $, and suppose that $ ax + by = d $ for some integers $ x $ and $ y $.

Let $ g = \gcd(x, y) $. Then $ g \mid x $ and $ g \mid y $, so $ g \mid ax + by = d $. Therefore, $ g \mid d $.

Now suppose $ g > 1 $. Then $ g \mid d $, and since $ d = \gcd(a, b) $, it follows that $ g \mid a $ and $ g \mid b $. Thus, $ g \mid a $ and $ g \mid b $, and $ g \mid x $, $ g \mid y $.

Let $ x = gx' $, $ y = gy' $. Then:

$$
ax + by = a(gx') + b(gy') = g(ax' + by') = d
$$

So, $ g(ax' + by') = d $, which implies:

$$
ax' + by' = \frac{d}{g}
$$

But since $ d = \gcd(a, b) $, we know that $ \frac{d}{g} $ must be an integer. However, $ ax' + by' = \frac{d}{g} $, and since $ g \mid d $, this implies that $ g $ must divide $ d $, and the only way this is possible is if $ g = 1 $.

Therefore, $ \gcd(x, y) = 1 $.

---

### **Final Answers**

(a) The statement is proven.  
(b) $ \boxed{1} $