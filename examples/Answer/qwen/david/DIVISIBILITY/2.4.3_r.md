<think>
Okay, so I need to prove that if d is a common divisor of a and b, then d equals the greatest common divisor of a and b (gcd(a, b)) if and only if the gcd of (a/d) and (b/d) is 1. Hmm, let me break this down.

First, let me recall what a common divisor is. A common divisor of two numbers a and b is a number that divides both a and b without leaving a remainder. The greatest common divisor (gcd) is the largest such number. So, if d is a common divisor, then d divides both a and b. The statement says that d is the gcd of a and b exactly when gcd(a/d, b/d) is 1. That seems to be saying that if you divide a and b by their common divisor d, then their resulting numbers are coprime, meaning their gcd is 1. 

Let me try to think of this in terms of definitions. Let me denote gcd(a, b) as g. So, if d is a common divisor of a and b, then d divides both a and b. The question is about when d is equal to g. The "if and only if" means I have to prove two directions: first, if d is the gcd, then gcd(a/d, b/d) is 1, and second, if gcd(a/d, b/d) is 1, then d is the gcd.

Starting with the first direction: Suppose d is the gcd of a and b. Then, since d is the greatest common divisor, any other common divisor of a and b must divide d. So, if I divide a and b by d, then a/d and b/d should have no common divisors other than 1. Because if there was a common divisor, say e, of a/d and b/d, then e*d would be a common divisor of a and b, which would contradict the fact that d is the greatest common divisor. Therefore, gcd(a/d, b/d) must be 1. That seems to make sense. Let me check with an example. Let’s say a = 8, b = 12. Then gcd(8,12) is 4. If d = 4, then a/d = 2, b/d = 3. gcd(2,3) is 1. Yep, that works. So that direction seems okay.

Now the other direction: Suppose that gcd(a/d, b/d) = 1. Then, we need to show that d is the gcd of a and b. Let me think. If gcd(a/d, b/d) is 1, then there are no common divisors of a/d and b/d except 1. Therefore, any common divisor of a and b must divide d. Because if there is a common divisor e of a and b, then e divides a and b. Then, since e divides a, e divides a = d*(a/d), so e divides d*(a/d). Similarly, e divides d*(b/d). Therefore, e divides d*(a/d) and d*(b/d). But since gcd(a/d, b/d) is 1, the only common divisor is 1. Therefore, e must divide d. Wait, but e divides d? Wait, maybe I need to think more carefully.

Let me suppose that e is a common divisor of a and b. Then, since e divides a and b, then e divides a and e divides b. But since d divides a and d divides b, then e divides d? Wait, no. Wait, if d is a common divisor of a and b, then e is another common divisor. So, if e divides both a and b, then e divides d? No, that's not necessarily true. Wait, but if d is a common divisor, then e divides d? Wait, no, if d is the gcd, then any common divisor divides d. But in this case, we are assuming that d is not necessarily the gcd. Wait, but in this direction, we are assuming that gcd(a/d, b/d) = 1, and we need to show that d is the gcd of a and b. So, let me think again.

If we have that gcd(a/d, b/d) = 1, then any common divisor of a and b must divide d. Because suppose e is a common divisor of a and b. Then e divides a and e divides b. But since d divides a and d divides b, then e divides d? Wait, how?

Let me think. Let me write a = d * (a/d) and b = d * (b/d). So, if e divides a and e divides b, then e divides d*(a/d) and e divides d*(b/d). Therefore, e divides d*(a/d) and e divides d*(b/d). Since gcd(a/d, b/d) = 1, then any common divisor of a/d and b/d is 1. Therefore, e must divide d. Because if e divides d*(a/d) and d*(b/d), but since a/d and b/d are coprime, then e can't have any common factors with them. Therefore, e must divide d. Therefore, e divides d. Therefore, the greatest common divisor of a and b must be d. Because any common divisor of a and b divides d, so d is the greatest common divisor. Therefore, that direction is proven.

Wait, let me check with an example. Suppose a = 8, b = 12. Let me take d = 4. Then a/d = 2, b/d = 3, gcd(2,3) = 1. Then, the gcd of a and b is 4. If I take another d, say d = 2. Then a/d = 4, b/d = 6. gcd(4,6) is 2, which is not 1. Therefore, according to the theorem, if gcd(a/d, b/d) is 1, then d is the gcd. So in this case, if we take d = 4, which is the gcd, then gcd(2,3) is 1. If we take d = 2, which is a common divisor, but gcd(4,6) is 2, which is not 1. Therefore, the theorem says that only when gcd(a/d, b/d) is 1, then d is the gcd. So that seems to hold.

Another example: Let a = 6, b = 9. gcd(6,9) is 3. If d = 3, then a/d = 2, b/d = 3, gcd(2,3) = 1. If I take d = 1, then a/d = 6, b/d = 9, gcd(6,9) = 3, which is not 1. Therefore, d = 1 is not the gcd. So that works. So if d is not the gcd, then gcd(a/d, b/d) is not 1. Therefore, the theorem seems to hold.

Therefore, the two directions are proven. Let me try to formalize this.

First, suppose that d is the gcd of a and b. Then, any common divisor of a and b must divide d. Therefore, if we divide a and b by d, then a/d and b/d have no common divisors except 1. Therefore, gcd(a/d, b/d) = 1.

Conversely, suppose that gcd(a/d, b/d) = 1. Then, any common divisor of a and b must divide d. Therefore, d is the greatest common divisor of a and b.

But wait, how do we know that d is the greatest? Because if any common divisor of a and b divides d, then d is the greatest. Since d is a common divisor, then it's the greatest. Therefore, that's the reasoning.

But let me try to write this more formally.

First, let me assume that d divides a and d divides b. Then, we need to show that d = gcd(a, b) if and only if gcd(a/d, b/d) = 1.

First direction: If d = gcd(a, b), then gcd(a/d, b/d) = 1.

Proof: Let g = gcd(a, b). Since d = g, then a = g * (a/g), b = g * (b/g). Therefore, a/d = a/g = (a/g), and similarly for b/d. Since g is the gcd, then gcd(a/g, b/g) = 1. Therefore, the statement holds.

Wait, but that seems too straightforward. Wait, but if d is the gcd, then a/d and b/d are coprime. Because if they had a common divisor e > 1, then e divides a/d and b/d, so e*d divides a and b, which would contradict that d is the gcd. Therefore, that's the reasoning.

Second direction: If gcd(a/d, b/d) = 1, then d = gcd(a, b).

Proof: Suppose that gcd(a/d, b/d) = 1. Let g = gcd(a, b). Then, since d divides a and d divides b, then d divides g. Therefore, d is a divisor of g. But since g is the gcd, then g divides d. Therefore, g = d. Wait, is that correct?

Wait, let me think again. Let me suppose that gcd(a/d, b/d) = 1. Let me denote that as e = gcd(a/d, b/d) = 1. Then, suppose that there is a common divisor of a and b, say k. Then, since k divides a and b, then k divides a = d*(a/d) and k divides b = d*(b/d). Therefore, k divides d*(a/d) and d*(b/d). Since gcd(a/d, b/d) = 1, then any common divisor of a/d and b/d is 1. Therefore, k divides d. Therefore, any common divisor of a and b divides d. Therefore, d is the greatest common divisor. Because d divides a and b, and any common divisor divides d. Therefore, d is the gcd. Therefore, that's the proof.

Alternatively, since if we have that any common divisor of a and b divides d, then d is the greatest common divisor. Because d is a common divisor, and any other common divisor divides d. Therefore, d is the greatest.

Therefore, the two directions are proven.

But let me check if there are any edge cases. For example, if a or b is zero. Wait, but the problem says that d is a common divisor of a and b. If a or b is zero, then the gcd is defined as the other number. But maybe we can assume that a and b are non-zero? Or maybe the problem is for positive integers? The problem didn't specify, but maybe we can assume that a and b are positive integers. Because gcd is usually defined for positive integers, but maybe in general.

But let's suppose that a and b are positive integers. Then, the reasoning holds. For example, if a = 0, then gcd(0, b) is |b|. But maybe the problem is intended for positive integers. Let me check the original problem statement. It just says "if d is a common divisor of a and b", so maybe a and b can be any integers, but d is a common divisor. However, in the case where a or b is zero, the gcd is defined as the other number. But maybe the problem is in the context of positive integers. Anyway, the proof seems to hold regardless.

Another edge case: if d = 1. Then, gcd(a, b) is 1 if and only if a and b are coprime. Then, if d = 1, then gcd(a/1, b/1) = gcd(a, b) = 1. Therefore, that's consistent. So if d is 1, then the condition is that gcd(a, b) is 1, which is equivalent to d = 1. Therefore, that's consistent.

Another example: Let a = 0, b = 0. Then, every integer is a common divisor, but the gcd is undefined or considered as 0. However, maybe the problem is not considering that case. But perhaps the problem is intended for non-zero integers.

Therefore, I think the proof is correct. The key idea is that if you divide a and b by their common divisor d, then the resulting numbers are coprime if and only if d is the greatest common divisor.

Therefore, the proof is as follows:

First, suppose that d is a common divisor of a and b. Then, d = gcd(a, b) if and only if gcd(a/d, b/d) = 1.

Proof:

(⇒) Suppose that d = gcd(a, b). Then, any common divisor of a and b must divide d. Therefore, if we divide a and b by d, then a/d and b/d have no common divisors other than 1. Hence, gcd(a/d, b/d) = 1.

(⇐) Conversely, suppose that gcd(a/d, b/d) = 1. Then, any common divisor of a and b must divide d. Because if e is a common divisor of a and b, then e divides a = d*(a/d) and b = d*(b/d). Since gcd(a/d, b/d) = 1, e must divide d. Therefore, d is the greatest common divisor of a and b.

Therefore, the statement is proven.

I think this covers both directions. The key is understanding that dividing a and b by d removes the common factors, and if there are no more common factors, then d is the greatest common divisor.

**Final Answer**
\boxed{1}
</think>

To prove the statement:

> If $ d $ is a common divisor of $ a $ and $ b $, then $ d = \gcd(a, b) $ if and only if $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $,

we analyze the relationship between the greatest common divisor (gcd) and the structure of the numbers when divided by a common divisor.

---

### **Understanding the Statement**

Let $ d $ be a common divisor of $ a $ and $ b $, meaning $ d \mid a $ and $ d \mid b $. We are to show that:

- **If** $ d = \gcd(a, b) $, then $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $,
- **And** if $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $, then $ d = \gcd(a, b) $.

This is a **biconditional** statement, and we will prove both directions.

---

### **Proof:**

#### **Direction 1: If $ d = \gcd(a, b) $, then $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $**

Let $ d = \gcd(a, b) $. Then:

- $ a = d \cdot x $, and $ b = d \cdot y $, for some integers $ x $ and $ y $.
- So, $ \frac{a}{d} = x $, and $ \frac{b}{d} = y $.
- Since $ d $ is the greatest common divisor of $ a $ and $ b $, any common divisor of $ x $ and $ y $ would imply a common divisor of $ a $ and $ b $ larger than $ d $, which is a contradiction.
- Therefore, $ \gcd(x, y) = 1 $, which means $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $.

**Hence, if $ d = \gcd(a, b) $, then $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $.**

---

#### **Direction 2: If $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $, then $ d = \gcd(a, b) $**

Suppose $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $. Then:

- Let $ a = d \cdot x $, $ b = d \cdot y $, so $ \frac{a}{d} = x $, $ \frac{b}{d} = y $, and $ \gcd(x, y) = 1 $.
- Let $ e $ be any common divisor of $ a $ and $ b $. Then $ e \mid a = d \cdot x $ and $ e \mid b = d \cdot y $.
- So, $ e \mid d \cdot x $ and $ e \mid d \cdot y $, and since $ \gcd(x, y) = 1 $, $ e $ must divide $ d $.
- Therefore, any common divisor of $ a $ and $ b $ divides $ d $, and since $ d \mid a $ and $ d \mid b $, it follows that $ d = \gcd(a, b) $.

**Hence, if $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $, then $ d = \gcd(a, b) $.**

---

### **Conclusion**

We have shown that:

- $ d = \gcd(a, b) \Rightarrow \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 $,
- $ \gcd\left(\frac{a}{d}, \frac{b}{d}\right) = 1 \Rightarrow d = \gcd(a, b) $.

Therefore, the statement is proven in both directions.

---

### **Final Answer**

$$
\boxed{1}
$$