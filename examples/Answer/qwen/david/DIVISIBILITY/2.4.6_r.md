<think>
Okay, so I need to prove that if the greatest common divisor of a and b is 1, then the greatest common divisor of a + b and ab is also 1. Hmm, let me start by recalling what gcd means. The gcd of two numbers is the largest number that divides both of them without leaving a remainder. So, if gcd(a, b) = 1, that means a and b share no common divisors other than 1. Now, I need to show that a + b and ab also share no common divisors other than 1.

Let me think about how to approach this. Maybe I can use the property that if a number divides both a + b and ab, then it must divide certain combinations of a and b. Let me try to formalize this.

Suppose there is a prime number p that divides both a + b and ab. Then, p divides ab, which means that p divides a or p divides b. Let me consider both cases.

Case 1: p divides a. If p divides a, then since p also divides a + b, it must divide (a + b) - a = b. So, p divides both a and b. But we know that gcd(a, b) = 1, which means that the only common divisor is 1. Therefore, p must be 1. However, since p is a prime number, the only way this can happen is if p = 1, but 1 isn't considered a prime. Wait, that seems contradictory. Maybe I need to adjust my reasoning here.

Hold on, maybe I should think in terms of prime factors. If p divides both a + b and ab, then p divides ab. So p divides a or p divides b. Let me suppose p divides a. Then p divides a + b, so p divides b. Therefore, p divides both a and b, which contradicts gcd(a, b) = 1. Therefore, such a prime p cannot exist. Hence, gcd(a + b, ab) must be 1.

Wait, but what if p is 1? But 1 is not a prime, so maybe I need to think about divisors in general, not just primes. Let me rephrase.

Suppose d is a common divisor of a + b and ab. Then d divides a + b and d divides ab. Let me try to show that d must be 1.

If d divides ab, then d divides a or d divides b. Let me suppose that d divides a. Then since d divides a + b, it must divide (a + b) - a = b. Therefore, d divides both a and b, which implies that d divides gcd(a, b) = 1. Therefore, d must be 1. Similarly, if d divides b, then by similar reasoning, d divides a, so again d divides 1. Therefore, d must be 1. Therefore, gcd(a + b, ab) = 1.

Wait, that seems to work. Let me check with an example. Let me take a = 2 and b = 3. Then gcd(2, 3) = 1. Then a + b = 5, and ab = 6. gcd(5, 6) is 1. That works.

Another example: a = 3, b = 4. gcd(3, 4) = 1. a + b = 7, ab = 12. gcd(7, 12) = 1. Okay. What about a = 1 and b = 1? gcd(1, 1) is 1. Then a + b = 2, ab = 1. gcd(2, 1) is 1. That works too.

What if a = 2 and b = 5? gcd(2, 5) = 1. a + b = 7, ab = 10. gcd(7, 10) = 1. Still works. How about a = 4 and b = 9? gcd(4, 9) = 1. a + b = 13, ab = 36. gcd(13, 36) = 1. Yep.

But let me think of a case where maybe there is a common divisor. Suppose a = 2, b = 3. Then a + b = 5, ab = 6. No common divisors. Suppose a = 3, b = 5. a + b = 8, ab = 15. gcd(8, 15) is 1. Okay.

Wait, maybe if a and b are both even? But if gcd(a, b) = 1, they can't both be even. Because if they were both even, then 2 would divide both, so gcd would be at least 2. So that's not possible. Therefore, in any case, if a and b are coprime, one of them is even and the other is odd? Not necessarily. For example, 3 and 4: one is odd, one is even. But 3 and 5 are both odd. But their gcd is 1. So, a + b would be even. But ab would be odd. So gcd(a + b, ab) would be gcd(even, odd) which is 1. So that's still okay.

Wait, so maybe there's no case where there is a common divisor. So the proof seems to hold. Let me check again the logic.

Assume that d divides both a + b and ab. Then, since d divides ab, it divides a or b. Suppose it divides a. Then since d divides a + b, it divides b. Therefore, d divides a and b, so d divides gcd(a, b) = 1. Therefore, d = 1. Therefore, gcd(a + b, ab) = 1. That seems solid.

Alternatively, maybe using the Euclidean algorithm. Since gcd(a + b, ab) = gcd(a + b, ab - a*(a + b)/something? Wait, maybe not. Let me think.

Alternatively, since gcd(a, b) = 1, then there exist integers x and y such that ax + by = 1. But not sure if that helps here.

Alternatively, think about prime factors. Suppose that there is a prime p dividing both a + b and ab. Then p divides ab, so p divides a or p divides b. Suppose p divides a. Then p divides a + b, so p divides b. Hence p divides both a and b, which contradicts gcd(a, b) = 1. Therefore, no such prime exists, so gcd(a + b, ab) = 1.

Yes, that's the same as before. Therefore, the conclusion is that gcd(a + b, ab) = 1.

Wait, but let me check if there's a case where maybe a + b and ab share a common factor. Suppose a = 1, b = 2. Then a + b = 3, ab = 2. gcd(3, 2) = 1. If a = 1, b = 3. Then a + b = 4, ab = 3. gcd(4, 3) = 1. If a = 1, b = 4. a + b = 5, ab = 4. gcd(5, 4) = 1. So seems okay.

Another case: a = 5, b = 7. a + b = 12, ab = 35. gcd(12, 35) = 1. If a = 5, b = 8. a + b = 13, ab = 40. gcd(13, 40) = 1. If a = 5, b = 12. gcd(5, 12) = 1. a + b = 17, ab = 60. gcd(17, 60) = 1. So seems like in all these cases, it's 1.

So, I think the proof is correct. The key idea is that if a prime divides ab, it divides a or b, and if it divides a, then it divides b, which contradicts the coprimality. Therefore, there are no common prime factors, so the gcd is 1.

Therefore, the conclusion is that if gcd(a, b) = 1, then gcd(a + b, ab) = 1.

**Final Answer**
\boxed{1}
</think>

To prove that if $\gcd(a, b) = 1$, then $\gcd(a + b, ab) = 1$, we proceed by examining the implications of the given condition and the properties of greatest common divisors (gcd).

---

### Step 1: Understanding the Given Condition

We are given that $\gcd(a, b) = 1$, meaning that $a$ and $b$ are **coprime**â€”they share no common prime factors other than 1.

---

### Step 2: Suppose a Common Divisor Exists

Let $d$ be a common divisor of both $a + b$ and $ab$. That is, we assume:

$$
d \mid a + b \quad \text{and} \quad d \mid ab
$$

We will show that this leads to a contradiction unless $d = 1$.

---

### Step 3: Use the Divisibility of $ab$

Since $d \mid ab$, it must divide at least one of $a$ or $b$, due to the fundamental property of divisors of a product.

- **Case 1:** Suppose $d \mid a$. Then, since $d \mid a + b$, we also have $d \mid (a + b) - a = b$. Therefore, $d \mid b$.
- **Case 2:** Suppose $d \mid b$. Then, since $d \mid a + b$, we also have $d \mid (a + b) - b = a$. Therefore, $d \mid a$.

In either case, we find that $d$ divides both $a$ and $b$, which implies:

$$
d \mid \gcd(a, b)
$$

But since $\gcd(a, b) = 1$, this means that $d = 1$.

---

### Step 4: Conclusion

The only positive integer that divides both $a + b$ and $ab$ is 1. Therefore, the greatest common divisor of $a + b$ and $ab$ must be 1.

$$
\gcd(a + b, ab) = 1
$$

---

### Final Answer

$$
\boxed{1}
$$