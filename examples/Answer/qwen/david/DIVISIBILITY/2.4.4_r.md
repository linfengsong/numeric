<think>
Okay, so I need to prove these four parts about gcds with the condition that gcd(a, b) = 1. Let me start with part (a): gcd(a + b, a - b) = 1 or 2. 

First, I remember that gcd(m, n) is the same as gcd(m, n - km) for any integer k. Maybe I can use that property here. Let me denote d = gcd(a + b, a - b). Then d divides both a + b and a - b. So, if d divides both, then it must divide their sum and difference. Let me compute the sum and difference.

Sum: (a + b) + (a - b) = 2a. Difference: (a + b) - (a - b) = 2b. Therefore, d divides 2a and 2b. But since gcd(a, b) = 1, then gcd(2a, 2b) = 2 * gcd(a, b) = 2 * 1 = 2. Therefore, d divides 2. Hence, d must be 1 or 2. That seems straightforward. Wait, but is there a case where d could be something else? For example, if a and b are both even? But since gcd(a, b) = 1, they can't both be even. So, if a and b are both odd, then a + b and a - b are both even, so their gcd is at least 2. If one is even and the other is odd, then a + b and a - b are both odd, so their gcd is 1. Therefore, the gcd is 1 or 2. So part (a) is done. 

Moving on to part (b): gcd(2a + b, a + 2b) = 1 or 3. Let me denote d = gcd(2a + b, a + 2b). Again, using the property that d divides any linear combination. Let me try to find a combination that eliminates one variable. Let me compute 2*(a + 2b) - (2a + b). That would be 2a + 4b - 2a - b = 3b. So d divides 3b. Similarly, compute 2*(2a + b) - (a + 2b) = 4a + 2b - a - 2b = 3a. Therefore, d divides 3a and 3b. Since gcd(a, b) = 1, then gcd(3a, 3b) = 3 * gcd(a, b) = 3 * 1 = 3. Therefore, d divides 3, so d is 1 or 3. 

But wait, is there a case where d could be 3? Let me check with an example. Suppose a = 1, b = 2. Then gcd(2*1 + 2, 1 + 2*2) = gcd(4, 5) = 1. Another example: a = 1, b = 1. Then gcd(2 + 1, 1 + 2) = gcd(3, 3) = 3. So that works. If a = 2, b = 1, then gcd(5, 4) = 1. If a = 1, b = 2, same as before. If a = 2, b = 1, same. If a = 1, b = 3, then gcd(2*1 + 3, 1 + 2*3) = gcd(5, 7) = 1. If a = 2, b = 3, then gcd(7, 8) = 1. If a = 1, b = 4, gcd(6, 9) = 3. So that works. Therefore, the gcd is 1 or 3. So part (b) is done.

Now part (c): gcd(a + b, a² + b²) = 1 or 2. Let me denote d = gcd(a + b, a² + b²). Since d divides a + b, let me write a ≡ -b mod d. Therefore, substituting into a² + b², we get (-b)² + b² = 2b². Therefore, d divides 2b². Similarly, since d divides a + b and gcd(a, b) = 1, then d must divide 2. Wait, let me think again. Since d divides a + b and gcd(a, b) = 1, then any common divisor of a + b and b must divide a. But since gcd(a, b) = 1, then gcd(a + b, b) = gcd(a, b) = 1. Therefore, d divides 2b² and gcd(d, b) = 1? Wait, no. Wait, if d divides a + b and d divides 2b², then since gcd(a + b, b) = 1, then gcd(d, b) must be 1. Therefore, d divides 2. Therefore, d is 1 or 2. 

Wait, but let me check with an example. Let me take a = 1, b = 2. Then a + b = 3, a² + b² = 5. gcd(3, 5) = 1. Another example: a = 1, b = 1. Then a + b = 2, a² + b² = 2. gcd(2, 2) = 2. If a = 2, b = 1, then a + b = 3, a² + b² = 5. gcd(3, 5) = 1. If a = 3, b = 1, then a + b = 4, a² + b² = 10. gcd(4, 10) = 2. If a = 2, b = 3, then a + b = 5, a² + b² = 13. gcd(5, 13) = 1. If a = 1, b = 3, then a + b = 4, a² + b² = 10. gcd(4, 10) = 2. So that seems to hold. Therefore, the gcd is 1 or 2. So part (c) is done.

Now part (d): gcd(a + b, a² - ab + b²) = 1 or 3. Let me denote d = gcd(a + b, a² - ab + b²). Again, since d divides a + b, then a ≡ -b mod d. Substitute into a² - ab + b². Let me compute that. Let me replace a with -b:

(-b)² - (-b)(b) + b² = b² + b² + b² = 3b². Therefore, d divides 3b². Similarly, since d divides a + b, and gcd(a, b) = 1, then gcd(d, b) must be 1. Because if a prime p divides d and b, then p divides a + b and b, so p divides a. But since gcd(a, b) = 1, p cannot divide both a and b. Therefore, gcd(d, b) = 1. Therefore, d divides 3. Therefore, d is 1 or 3. 

But wait, let me check with examples. Let me take a = 1, b = 2. Then a + b = 3, a² - ab + b² = 1 - 2 + 4 = 3. So gcd(3, 3) = 3. Another example: a = 1, b = 1. Then a + b = 2, a² - ab + b² = 1 - 1 + 1 = 1. gcd(2, 1) = 1. If a = 2, b = 1, then a + b = 3, a² - ab + b² = 4 - 2 + 1 = 3. gcd(3, 3) = 3. If a = 2, b = 3, then a + b = 5, a² - ab + b² = 4 - 6 + 9 = 7. gcd(5, 7) = 1. If a = 1, b = 3, then a + b = 4, a² - ab + b² = 1 - 3 + 9 = 7. gcd(4, 7) = 1. If a = 3, b = 6, but wait, gcd(3, 6) is 3, which is not 1. So that's not allowed. Let me take a = 2, b = 5. Then a + b = 7, a² - ab + b² = 4 - 10 + 25 = 19. gcd(7, 19) = 1. If a = 1, b = 2, we already saw 3. If a = 1, b = 4, then a + b = 5, a² - ab + b² = 1 - 4 + 16 = 13. gcd(5, 13) = 1. If a = 2, b = 5, as above. If a = 3, b = 2, then a + b = 5, a² - ab + b² = 9 - 6 + 4 = 7. gcd(5, 7) = 1. 

But wait, what if a and b are such that a + b is divisible by 3? For example, a = 1, b = 2. Then a + b = 3, and a² - ab + b² = 3, so gcd is 3. If a = 2, b = 1, same. If a = 4, b = 5, then a + b = 9, a² - ab + b² = 16 - 20 + 25 = 21. gcd(9, 21) = 3. So that works. So in cases where 3 divides a + b, then 3 divides a² - ab + b². Therefore, the gcd is 3. If 3 does not divide a + b, then the gcd is 1. Therefore, the gcd is 1 or 3. Hence, part (d) is done.

Wait, but let me check if there's a case where d could be 3 even if 3 divides a + b. For example, if a + b is 3, then a² - ab + b² is 3. So gcd(3, 3) = 3. If a + b is 6, then a and b are such that they add to 6. Let me take a = 1, b = 5. Then a + b = 6. a² - ab + b² = 1 - 5 + 25 = 21. gcd(6, 21) = 3. So that works. So even if a + b is a multiple of 3, the gcd is 3. Therefore, the conclusion is correct.

So all parts are done. Let me just recap:

(a) Using the fact that gcd(a + b, a - b) divides 2a and 2b, and since gcd(a, b) = 1, the gcd is 1 or 2.

(b) Similarly, using linear combinations to show that the gcd divides 3a and 3b, hence 3, so 1 or 3.

(c) By substituting a ≡ -b mod d, leading to d divides 2b², and since gcd(d, b) = 1, d divides 2.

(d) Similarly, substituting a ≡ -b mod d, leading to d divides 3b², and since gcd(d, b) = 1, d divides 3.

Therefore, all parts are proven.

**Final Answer**

(a) \boxed{1} or \boxed{2}

(b) \boxed{1} or \boxed{3}

(c) \boxed{1} or \boxed{2}

(d) \boxed{1} or \boxed{3}
</think>

To prove the given statements under the condition that $\gcd(a, b) = 1$, we use properties of the greatest common divisor (gcd), particularly the fact that any common divisor of two numbers also divides their linear combinations. We analyze each part step by step.

---

### **(a) $\gcd(a + b, a - b) = 1$ or $2$**

Let $d = \gcd(a + b, a - b)$. Then $d$ divides both $a + b$ and $a - b$. Adding and subtracting these, we get:

- $d \mid (a + b) + (a - b) = 2a$
- $d \mid (a + b) - (a - b) = 2b$

So $d \mid \gcd(2a, 2b) = 2 \cdot \gcd(a, b) = 2$. Therefore, $d$ must divide 2. Since $d$ is a positive integer, it can be either 1 or 2.

**Conclusion:** $\gcd(a + b, a - b) = 1$ or $2$.

$$
\boxed{1} \text{ or } \boxed{2}
$$

---

### **(b) $\gcd(2a + b, a + 2b) = 1$ or $3$**

Let $d = \gcd(2a + b, a + 2b)$. Using linear combinations:

- $2(a + 2b) - (2a + b) = 3b$
- $2(2a + b) - (a + 2b) = 3a$

Thus, $d \mid 3a$ and $d \mid 3b$. Since $\gcd(a, b) = 1$, we have $\gcd(3a, 3b) = 3$. Therefore, $d \mid 3$, and so $d$ must be 1 or 3.

**Conclusion:** $\gcd(2a + b, a + 2b) = 1$ or $3$.

$$
\boxed{1} \text{ or } \boxed{3}
$$

---

### **(c) $\gcd(a + b, a^2 + b^2) = 1$ or $2$**

Let $d = \gcd(a + b, a^2 + b^2)$. Since $d \mid a + b$, we can write $a \equiv -b \mod d$. Substituting into $a^2 + b^2$, we get:

$$
a^2 + b^2 \equiv (-b)^2 + b^2 = 2b^2 \mod d
$$

So $d \mid 2b^2$. Now, since $d \mid a + b$ and $\gcd(a, b) = 1$, any common divisor of $d$ and $b$ must divide $a$, which is impossible. Hence, $\gcd(d, b) = 1$, so $d \mid 2$.

Therefore, $d$ must be 1 or 2.

**Conclusion:** $\gcd(a + b, a^2 + b^2) = 1$ or $2$.

$$
\boxed{1} \text{ or } \boxed{2}
$$

---

### **(d) $\gcd(a + b, a^2 - ab + b^2) = 1$ or $3$**

Let $d = \gcd(a + b, a^2 - ab + b^2)$. Since $d \mid a + b$, we can write $a \equiv -b \mod d$. Substituting into $a^2 - ab + b^2$, we get:

$$
a^2 - ab + b^2 \equiv (-b)^2 - (-b)b + b^2 = b^2 + b^2 + b^2 = 3b^2 \mod d
$$

So $d \mid 3b^2$. Again, since $\gcd(a, b) = 1$, any common divisor of $d$ and $b$ must divide $a$, which is impossible. So $\gcd(d, b) = 1$, and thus $d \mid 3$.

Therefore, $d$ must be 1 or 3.

**Conclusion:** $\gcd(a + b, a^2 - ab + b^2) = 1$ or $3$.

$$
\boxed{1} \text{ or } \boxed{3}
$$

---

### Final Answers

(a) $\boxed{1}$ or $\boxed{2}$  
(b) $\boxed{1}$ or $\boxed{3}$  
(c) $\boxed{1}$ or $\boxed{2}$  
(d) $\boxed{1}$ or $\boxed{3}$