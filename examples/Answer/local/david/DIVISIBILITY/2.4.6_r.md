 To prove that if gcd(a, b) = 1, then gcd(a + b, ab) = 1, we will use the property of greatest common divisors (gcd) and the definition of coprime numbers.

First, let's recall the definition of gcd(a, b) = 1. This means that the greatest common divisor of a and b is 1, which implies that a and b are coprime, i.e., they have no common factors other than 1.

Now, let's consider gcd(a + b, ab). We want to show that this is also equal to 1.

Suppose, for the sake of contradiction, that gcd(a + b, ab) ≠ 1. This means that a + b and ab have a common factor greater than 1. Let's call this common factor d, so d > 1.

Since d is a common factor of a + b and ab, we can write:

a + b = dk (1)
ab = dl (2)

for some integers k and l.

Now, let's multiply equation (1) by a and equation (2) by b:

a(a + b) = adk
b(ab) = bdl

Rearranging the terms, we get:

a^2 + ab = adk
ab^2 = bdl

Now, let's subtract the second equation from the first:

a^2 + ab - ab^2 = adk - bdl
a^2 - ab^2 + ab = d(ak - bl)

Factoring the left side, we get:

a(a - b^2) + ab = d(ak - bl)

Now, notice that a and b are coprime, so a and b^2 are also coprime. Therefore, a and a - b^2 are also coprime. This means that a(a - b^2) and ab are also coprime.

Since a(a - b^2) and ab are coprime, and their gcd is d, which is greater than 1, this contradicts our assumption that gcd(a + b, ab) ≠ 1.

Therefore, our assumption that gcd(a + b, ab) ≠ 1 must be false, and we conclude that gcd(a + b, ab) = 1.