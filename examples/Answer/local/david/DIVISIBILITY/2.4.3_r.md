 To prove this, we will use the definition of the greatest common divisor (gcd) and the properties of common divisors.

First, let's recall the definition of the gcd:

The greatest common divisor (gcd) of two integers a and b, denoted as gcd(a, b), is the largest positive integer that divides both a and b without leaving a remainder.

Now, let's consider the given statement:

If d is a common divisor of a and b, then d = gcd(a, b) if and only if gcd(a/d, b/d) = 1.

We will prove this statement by breaking it down into two parts:

1. If d = gcd(a, b), then gcd(a/d, b/d) = 1.
2. If gcd(a/d, b/d) = 1, then d = gcd(a, b).

Let's start with the first part:

1. If d = gcd(a, b), then gcd(a/d, b/d) = 1.

If d = gcd(a, b), then d is the largest positive integer that divides both a and b. This means that a/d and b/d are integers, and there are no common divisors other than 1. Therefore, gcd(a/d, b/d) = 1.

Now, let's move on to the second part:

2. If gcd(a/d, b/d) = 1, then d = gcd(a, b).

If gcd(a/d, b/d) = 1, then there are no common divisors other than 1 for a/d and b/d. This means that d is the largest positive integer that divides both a and b. Therefore, d = gcd(a, b).

Combining the results from both parts, we have proven that if d is a common divisor of a and b, then d = gcd(a, b) if and only if gcd(a/d, b/d) = 1.