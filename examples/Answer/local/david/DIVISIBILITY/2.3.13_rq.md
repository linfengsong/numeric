<think>
Okay, so I need to prove these two statements about integers a, b, c, and their greatest common divisors. Let me start with part (a). The question says that there exist integers x and y such that c = ax + by if and only if gcd(a, b) divides c. Hmm, okay. Let me recall some number theory concepts here. 

First, I remember that the greatest common divisor of two numbers a and b, denoted gcd(a, b), is the largest integer that divides both a and b. Also, there's the concept of linear combinations. The theorem that comes to mind is the linear combination theorem, which states that the set of all linear combinations of a and b is exactly the set of multiples of gcd(a, b). So, that is, for integers a and b, any linear combination ax + by is a multiple of gcd(a, b), and conversely, every multiple of gcd(a, b) can be expressed as a linear combination of a and b. 

So, if that's the case, then part (a) is essentially stating that c can be written as ax + by if and only if gcd(a, b) divides c. Which seems to align with the theorem. But I need to write a proper proof for this. Let me try to break it down into two parts: the "if" direction and the "only if" direction.

Starting with the "if" direction: Suppose that gcd(a, b) divides c. Then, there exists some integer k such that c = k * gcd(a, b). Now, since gcd(a, b) divides both a and b, we can write gcd(a, b) as a linear combination of a and b. Wait, actually, the theorem says that there exist integers x and y such that gcd(a, b) = ax + by. So, if c is k times that, then c = k*(ax + by) = (kx)*a + (ky)*b. Therefore, x' = kx and y' = ky are integers, so we have c = a x' + b y'. Therefore, the "if" direction holds. 

Now the "only if" direction: Suppose that there exist integers x and y such that c = ax + by. Then, we need to show that gcd(a, b) divides c. Let me think. Since gcd(a, b) divides both a and b, it divides any linear combination of a and b. Therefore, gcd(a, b) divides ax + by, which is c. Therefore, gcd(a, b) divides c. 

So, that seems straightforward. Wait, but is there a more formal way to write this? Let me check. Let me denote d = gcd(a, b). Then, d divides a and d divides b. Therefore, d divides any linear combination of a and b, so d divides c. Hence, the "only if" direction is proven. 

Therefore, combining both directions, the statement holds. Therefore, part (a) is proven. 

Wait, but maybe I need to be more precise. Let me check if the converse is also true. Suppose that d divides c, so c = d * k for some integer k. Then, since d can be written as ax + by for some integers x and y (by the Bézout's identity), then c = d*k = (ax + by)*k = a*(xk) + b*(yk), so x' = xk and y' = yk are integers, hence c = ax' + by'. Therefore, that's the "if" direction. 

Yes, so that's the proof. Therefore, part (a) is done. 

Now, moving on to part (b): If there exist integers x and y for which ax + by = gcd(a, b), then gcd(x, y) = 1. 

Hmm, okay. So, given that ax + by equals the gcd of a and b, then the gcd of x and y is 1. Let me think. Let me denote d = gcd(a, b). So, we have ax + by = d. Now, we need to show that gcd(x, y) = 1. 

Let me suppose that there is a common divisor of x and y, say g. So, g divides x and g divides y. Then, since g divides x and y, it divides any linear combination of x and y. In particular, g divides ax + by, which is d. Therefore, g divides d. But d is the gcd of a and b, so the divisors of d are the common divisors of a and b. But since g divides x and y, and g divides d, then g divides d. However, we need to show that g is 1. 

Wait, but how do we know that there isn't a larger g? Let me think. Suppose that g is a common divisor of x and y. Then, g divides d. But since d = gcd(a, b), then g divides d. But how do we know that g can't be greater than 1? 

Wait, maybe we need to show that the gcd of x and y divides d, but also that d divides something? Wait, perhaps we need to find a relation between gcd(x, y) and d. Let me think again. 

Alternatively, suppose that gcd(x, y) = g. Then, g divides x and y. Therefore, as above, g divides ax + by = d. Therefore, g divides d. So, g is a divisor of d. But we need to show that g is 1. 

Wait, but how? Unless there's some other condition. Let me think. Maybe we can use the fact that d is the gcd of a and b. Suppose that g divides both x and y. Then, since g divides d, which is the gcd(a, b), then g divides a and g divides b? Wait, no. Wait, d divides a and d divides b. If g divides d, then g divides a and g divides b? No, that's not necessarily true. Wait, if g divides d, then since d divides a and d divides b, then g divides a and g divides b? Wait, no. For example, suppose d = 6, and g = 2. Then, 2 divides 6, but 2 divides a and b? Not necessarily. For example, suppose a = 6 and b = 6, then d = 6. If g = 2 divides d, then 2 divides a and 2 divides b? Well, yes, in this case, but if a and b were different? Suppose a = 6 and b = 3, then d = 3. If g divides d = 3, say g = 3. Then, 3 divides a = 6 and 3 divides b = 3. But if a = 6 and b = 4, then d = 2. Then, if g divides d = 2, say g = 2, then 2 divides a = 6 and 2 divides b = 4. So, in general, if g divides d, which divides a and b, then g divides a and g divides b? Wait, no. Wait, if d divides a and d divides b, then any divisor of d divides a and divides b. Because if d divides a, then any divisor of d divides a. Similarly for b. Therefore, if g divides d, then g divides a and g divides b. 

Wait, so if g divides d, then g divides a and g divides b. Therefore, if we have that g divides x and y, then g divides d, so g divides a and g divides b. But then, since ax + by = d, then g divides ax + by = d. But since g divides a and g divides b, then g divides ax and g divides by, so g divides ax + by, which is d. Therefore, that's consistent. But how does that help us? 

Wait, but we need to show that the gcd(x, y) is 1. So, if we assume that there exists a common divisor g of x and y, then g divides d. But how do we know that g cannot be greater than 1? 

Wait, unless there is some other condition. Let me think again. Suppose that d = gcd(a, b). Let me consider the equation ax + by = d. Suppose that there is a common divisor g of x and y. Then, as above, g divides d. But if we can show that g divides 1, then g must be 1. How? 

Wait, maybe we can use the fact that d = gcd(a, b). So, if g divides d, then g divides a and g divides b. But since ax + by = d, then if we have that g divides a and g divides b, then g divides d. But how does that relate to x and y? 

Alternatively, maybe we can consider the equation ax + by = d. If we divide both sides by d, we get (a/d)x + (b/d)y = 1. Since d = gcd(a, b), then a/d and b/d are coprime. Because if there was a common divisor of a/d and b/d, say k, then k*d would divide both a and b, contradicting that d is the greatest common divisor. Therefore, a/d and b/d are coprime. 

So, the equation becomes (a/d)x + (b/d)y = 1. Let me denote A = a/d and B = b/d. Then, A and B are coprime integers, and we have A x + B y = 1. Now, we need to show that gcd(x, y) = 1. 

So, suppose that there exists a common divisor g of x and y. Then, g divides x and g divides y. Therefore, g divides A x + B y = 1. Therefore, g divides 1. Therefore, the only possible divisors are ±1. Hence, gcd(x, y) must be 1. 

Ah! There we go. Therefore, if we can write the equation as A x + B y = 1 with A and B coprime, then any common divisor of x and y must divide 1, hence gcd(x, y) = 1. 

Therefore, that shows that gcd(x, y) = 1. 

Wait, so this approach seems better. Let me recap. If we have ax + by = d, where d = gcd(a, b), then dividing both sides by d gives (a/d)x + (b/d)y = 1. Since a/d and b/d are coprime, then any common divisor of x and y would divide the left-hand side, which is 1. Therefore, their gcd must be 1. 

Therefore, part (b) is proven. 

But let me check if there's another way to approach this. Suppose that we don't divide by d. Let me try again. Suppose that there exists a common divisor g of x and y. Then, as before, g divides ax + by = d. So, g divides d. Therefore, g divides d. Also, since g divides x and y, then we can write x = g x' and y = g y'. Then, substituting into ax + by = d, we get a g x' + b g y' = d, which is g (a x' + b y') = d. Therefore, g divides d. But since d is the gcd(a, b), then g divides d. However, we can write d = g * k for some integer k. Therefore, substituting back, we have g (a x' + b y') = g * k, which implies that a x' + b y' = k. 

But since d = gcd(a, b), and k = d / g, then k is an integer. However, I don't see how this helps us. Unless we can find that k must be 1? Wait, but how? 

Wait, unless we can show that k must be 1. But how? 

Wait, but if we consider that a x' + b y' = k. But since a and b have gcd d, then the equation a x' + b y' = k implies that d divides k. Therefore, since k = d / g, then d divides d/g, which implies that g divides 1. Therefore, g divides 1, so g is ±1. Therefore, gcd(x, y) = |g| = 1. 

Wait, that seems like a valid approach. Let me check. 

So, if we suppose that g divides x and y, then we have that g divides d. Let me write x = g x', y = g y'. Then, substituting into ax + by = d gives a g x' + b g y' = d => g (a x' + b y') = d. Therefore, since d = gcd(a, b), then the left-hand side is g times something, and the right-hand side is d. Therefore, g divides d. So, d = g * k for some integer k. Therefore, substituting back, we have g (a x' + b y') = g * k => a x' + b y' = k. 

Now, since k = d / g, and d = gcd(a, b), then k must be an integer. But also, since a x' + b y' = k, and since gcd(a, b) = d, then d divides k. Therefore, d divides k. But k = d / g, so d divides d / g. Therefore, d divides d / g => g divides 1. Therefore, g divides 1. Therefore, the only possible values of g are 1 and -1, hence the gcd of x and y is 1. 

Therefore, this approach also works. 

So, either way, whether by dividing by d and using coprimality, or by substituting and showing that g divides 1, we can conclude that gcd(x, y) = 1. 

Therefore, part (b) is proven. 

Let me check with an example to make sure. Suppose a = 4, b = 6. Then, gcd(4, 6) = 2. Suppose we have ax + by = 2. For example, 4*1 + 6*(-1) = 4 - 6 = -2. But we can take x = 1, y = -1. Then, gcd(1, -1) = 1. Another example: 4*(-1) + 6*1 = -4 + 6 = 2. So x = -1, y = 1. gcd(-1, 1) = 1. 

Another example: suppose a = 5, b = 10. gcd(5, 10) = 5. Then, 5*1 + 10*0 = 5. So x = 1, y = 0. gcd(1, 0) = 1. Wait, gcd(1, 0) is 1, since gcd(n, 0) = |n|. So yes, that works. 

Another example: suppose a = 3, b = 5. Then, gcd(3,5) = 1. So, 3*2 + 5*(-1) = 6 -5 = 1. So x = 2, y = -1. gcd(2, -1) = 1. 

If I take x = 2 and y = -1, then their gcd is 1. So, that works. 

What if there was a case where x and y had a common divisor greater than 1? Suppose someone claims that there exists x and y such that ax + by = gcd(a, b) with gcd(x, y) > 1. But according to our proof, that's impossible. Let me try to find such a case. 

Suppose a = 6, b = 9. Then, gcd(6, 9) = 3. Suppose we have 6x + 9y = 3. Dividing by 3: 2x + 3y = 1. Let me find integers x and y. For example, x = 2, y = -1: 2*2 + 3*(-1) = 4 - 3 = 1. So x = 2, y = -1. gcd(2, -1) = 1. Another solution: x = -1, y = 1: 2*(-1) + 3*1 = -2 + 3 = 1. gcd(-1, 1) = 1. 

Alternatively, suppose someone tried to find x and y such that 6x + 9y = 3 with gcd(x, y) > 1. Let me suppose that x and y have a common divisor g > 1. Then, 6x + 9y = 3 => 2x + 3y = 1. Suppose that g divides x and y. Then, g divides 2x + 3y = 1. Therefore, g divides 1, so g = 1. Therefore, impossible. Therefore, no such x and y with gcd(x, y) > 1. 

Therefore, the example checks out. 

Therefore, both parts (a) and (b) are proven. 

**Final Answer**

(a) $\boxed{\text{Proven}}$  
(b) $\boxed{1}$
</think>

To prove the two statements regarding integer linear combinations and the greatest common divisor (gcd), we proceed with a structured and rigorous approach for each part.

---

### **(a) There exist integers $ x $ and $ y $ for which $ c = ax + by $ if and only if $ \gcd(a, b) \mid c $**

#### **Proof:**

Let $ d = \gcd(a, b) $. We will prove this in two directions.

---

#### **(i) If $ \gcd(a, b) \mid c $, then there exist integers $ x, y $ such that $ c = ax + by $:**

By **Bézout's Identity**, we know that there exist integers $ x_0, y_0 $ such that:

$$
d = ax_0 + by_0
$$

If $ c = k \cdot d $ for some integer $ k $, then:

$$
c = k \cdot (ax_0 + by_0) = a(kx_0) + b(ky_0)
$$

Letting $ x = kx_0 $, $ y = ky_0 $, we have:

$$
c = ax + by
$$

Hence, such integers $ x $ and $ y $ exist.

---

#### **(ii) If $ c = ax + by $, then $ \gcd(a, b) \mid c $:**

Since $ d = \gcd(a, b) $, it divides both $ a $ and $ b $. Therefore, $ d $ divides any linear combination of $ a $ and $ b $. In particular, it divides