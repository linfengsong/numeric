 To show that gcd(a, 0) = |a|, we need to show that |a| is the greatest common divisor of a and 0.

By definition, a number d is a common divisor of a and 0 if and only if d divides both a and 0. Since 0 is divisible by any integer, including |a|, we know that |a| is a common divisor of a and 0.

Now, we need to show that |a| is the greatest common divisor. Suppose there exists a larger common divisor d of a and 0. Then d must divide 0, which implies that d = 0 or d = |a|. However, since a is nonzero, |a| is not equal to 0, so d cannot be equal to 0. Therefore, the only possible value for d is |a|, which means that |a| is the greatest common divisor of a and 0.

Next, we show that gcd(a, a) = |a|. Since a divides a, |a| is a common divisor of a and a. Moreover, if d is any common divisor of a and a, then d must divide a, which implies that d divides |a|. Therefore, |a| is the greatest common divisor of a and a.

Finally, we show that gcd(a, 1) = 1. Since 1 divides a, 1 is a common divisor of a and 1. Moreover, if d is any common divisor of a and 1, then d must divide 1, which implies that d = 1. Therefore, 1 is the greatest common divisor of a and 1.